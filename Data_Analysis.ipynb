{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Oliviero Andreussi, olivieroandreuss@boisestate.edu\n",
    "\n",
    "Boise State University, Department of Chemistry and Biochemistry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis: Plots, Errors, Fits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us import some of the main modules that we will need for this lecture. These modules have already been introduced in the previous lecture. However, in the following we will introduce some new modules, we will add more details about them in the right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now specify the local path to the folder containing your data files. Remember to put a '/' at the end of the path and double check that the path looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/content/gdrive/MyDrive/' # this is the default path of your google drive\n",
    "my_path = 'Colab Notebooks/Test_Files/' # make sure you change this to the correct path of the folder with the files\n",
    "path = base_path + my_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Make a Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check that the data was read correctly. This usually involve looking at the head, tail, and info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plot using matplotlib. Remember to add axes labels using `plt.xlabel` and `plt.ylabel`. Remember to add the units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.plot(data['Time2'],data['Absorbance2'])\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify a range for x or y (zoom in or out) using `plt.xlim` and `plt.ylim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.xlim(1,4) # from 1 to 4 seconds\n",
    "plt.ylim(0.,0.3) # correspondent zoom on the y axis\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to use a log scale for the vertical axis, add a legend, add markers to the lines, specify the color of the curve and the size of the marker, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['Time'],data['Absorbance'],'o-',ms=1,color='orange',label='Set A') # semilogy uses a log scale for the y axis\n",
    "#'o-' means a round marker and a continuous line \n",
    "# ms controls the marker size\n",
    "# color is pretty intuitive\n",
    "# label adds a text to the curve\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.legend() # this will add a legend with all the specified labels\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Have Multiple Plots in the Same Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Step 1 to 3 from above and make sure the individual plots are what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create multiple plots using the `plt.subplot(n,m,i)`. The command expects three integers, which specify how many rows (`m`) and columns (`n`) of subplots we want, as well as which subplot (`i`) we want to work on (ranging from 1 to m*n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)) # if you have multiple plots you may want to change the overall dimensions of the figure\n",
    "plt.subplot(2,1,1) # 2 rows and 1 column of plots, working on the first one (top)\n",
    "plt.semilogy(data['Time'],data['Absorbance'], label='Absorbance') # semilogy uses a log scale for the y axis\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2) # 2 rows and 1 column of plots, working on the second plot (bottom)\n",
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Report Measurement Estimates and Confidence Intervals for Large Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple sources of errors affect our measurements. If these sources of errors are small and random, and in the limit of an infinite number of measurements, the results will end up being distributed according to a bell curve (a.k.a. a Gaussian or a normal distribution). The exact value will correspond to the center of the Gaussian, usually indicated by the parameter $\\mu$. The Central Limit Theorem tells us that the best estimate for our quantity is represented by the mean of the repeated measures, i.e.,  $m=\\frac{1}{N}\\sum x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ages.csv'\n",
    "ages = pd.read_csv(path+file,names=['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check your data, print it, look at its statistical descriptors, plot an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ages.head())\n",
    "print(ages.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ages,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the mean and standard deviation of your sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample of measurements is not really symmetric or looking like a Gaussian, that's because we only have a finite number $N$ of measurements. This means that using the sample's mean $m$ as our estimate for the center of the Gaussian $\\mu$ comes with an error. The Central Limit Theorem tells us that the Standard Error of the Mean (SEM, or $\\sigma_m$) is given by $\\sigma_m$=$\\sigma/\\sqrt{N}$, where $\\sigma$ is the spread of our Gaussian distribution. Like for the center of the distribution, we can estimate this spread from the sample's data, $s^2=\\frac{1}{N-1}\\sum (x_i-m)^2$. Since we already used the data to estimate $\\mu$, we have one less degree of freedom at the denominator (in practice this tells us that we cannot compute the spread if we have only one datapoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method provides immediate access to both the mean and the standard deviation of the sample. We can also compute them using methods of numpy arrays as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sample = ages['age'].mean()\n",
    "std_sample = ages['age'].std(ddof=1) # ddof, or delta degree of freedom, is needed to have the -1 at the denominator\n",
    "N_sample = len(ages['age'])\n",
    "print(mean_sample,std_sample,N_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute the standard deviation of the mean from the numbers above or the ones returned by `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM = std_sample/np.sqrt(N_sample)\n",
    "print(SEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but note that there is also a dedicated function in the SciPy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "sem(ages['age']) # NOTE: sem() can work on a np.array or on a column of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Identify the critical value for the chosen confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem tells us that the mean of the sample is also distributed according to a Gaussian. The confidence interval (CI) that corresponds to a confidence level $\\gamma$ of 95% tells us that 95% of the area of the Gaussian is contained between the CI values. While the CI of a Gaussian curve will depend on the mean and standard deviation of the specific Gaussian, we can use the properties of a normalized Gaussian centered on the zero (the z-distribution) and express the CI of our estimate in terms of a critical value, i.e., the corresponding interval for the z-distribution. In practice, $CI = m\\pm z_\\gamma \\sigma_m$, where $\\sigma_m$ is the SEM, and $z_\\gamma$ is the critical value that corresponds to the given confidence level. Typical confidence levels and corresponding critical values are: \n",
    "* 90% -> $z_{90}=1.64$\n",
    "* 95% -> $z_{95}=1.96$\n",
    "* 99% -> $z_{99}=2.58$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy also provides an easy way to compute critical values for arbitrary confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "confidence = 0.9999\n",
    "norm.ppf((1+confidence)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Compute the error of your measure and fix the significant figures accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "error_ci95 = sem(ages['age'])*norm.ppf((1+confidence)/2)\n",
    "print(error_ci95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The measured age is {mean_sample:4.1f} \\u00B1 {error_ci95:3.1f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Report Measurement Estimates and Confidence Intervals for Small Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a small sample, say 3 measurements instead of 20, instead of using the z-distribution one needs to use the Student's t-distribution. Steps 1 to 4 of the section above stay the same, but Step 5 now requires to identify a specific critical value. The additional challenge is the fact that the t-distribution depends on one additional parameter, the degrees of freedom of the sample (for our applications $N-1$). While normally you would have to look up critical values of t statistics on ugly tables, SciPy allows us to compute them on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "confidence = 0.95\n",
    "error_ci95 = sem(ages['age'])*t.ppf((1+confidence)/2,df=(len(ages)-1))\n",
    "print(error_ci95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The measured age is {mean_sample:2.0f} \\u00B1 {error_ci95:1.0f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval using t-distribution will always be larger than the one of a Gaussian, but the two will become more and more similar as the number of samples gets larger. In our example the difference is minimal, although it makes us report a bigger error because of rounding. Usually less than 10 samples will show a significant difference between the two approaches. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Propagate Errors for Functions of One Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simpler case of a function of a single random variable, $y=f(x)$, one can compute the spread on the result as $\\sigma_y = \\left|\\frac{df}{dx}\\right|\\sigma_x$. What is the origin of this formula?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions to load the data { display-mode: \"form\" }\n",
    "import sympy as sp\n",
    "x = sp.symbols('x')\n",
    "function = 'x**2' # @param {type:\"string\"}\n",
    "derivative = sp.diff(function,x)\n",
    "n_measures=10000\n",
    "mean_x = 2 # @param {type:\"number\"}\n",
    "std_x = 0.4 # @param {type:\"number\"}\n",
    "mu_x = mean_x\n",
    "sigma_x = std_x\n",
    "X = np.random.normal(mu_x,sigma_x,n_measures)\n",
    "funct = sp.lambdify(x, function, \"numpy\")  \n",
    "Y = funct(X)\n",
    "dfunct = sp.lambdify(x, derivative, \"numpy\")\n",
    "error_propagation = dfunct(X.mean())*X.std(ddof=1)\n",
    "print(\"The spread on the result is {:0.3f}, while the error propagation formula gives us {:.3f}\".format(Y.std(ddof=1),error_propagation))\n",
    "# Create a Figure, which doesn't have to be square.\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "# Create the main axes, leaving 25% of the figure space at the top and on the\n",
    "# right to position marginals.\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "# Create the Axes.\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "# Draw the scatter plot and marginals.\n",
    "ax.scatter(X,Y,alpha=0.1)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histx.hist(X,bins=100)\n",
    "\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "ax_histy.hist(Y, bins=100, orientation='horizontal')\n",
    "\n",
    "x=np.linspace(X.min(),X.max(),100)\n",
    "y=funct(X.mean())+dfunct(X.mean())*(x-X.mean())\n",
    "ax.plot(x,y,'red')\n",
    "\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()-X.std(), funct(X.mean()-X.std())), xycoords='data',\n",
    "            xytext=(X.mean()-X.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()+X.std(), funct(X.mean()+X.std())), xycoords='data',\n",
    "            xytext=(X.mean()+X.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()-X.std(), funct(X.mean()-X.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(X.mean()-X.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()+X.std(), funct(X.mean()+X.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(X.mean()+X.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know how to compute the derivative of your function with respect to the random variable, you should: \n",
    "1. Refresh your Calculus (always a great idea)\n",
    "\n",
    "or\n",
    "\n",
    "2. Use SymPy, a symbolic python module that can compute the formula of the derivative for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.symbols('x')\n",
    "function = 'x**2' # you can write the expression of your function as a string\n",
    "derivative = sp.diff(function,x) # this gets the analytical expression of df/dx\n",
    "print(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `function` and `derivative` above are just strings, `SymPy` can convert them to Numpy functions, so you can use them to propagate your errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.symbols('x')\n",
    "function = 'x**2' \n",
    "derivative = sp.diff(function,x) \n",
    "mu_x = 2.\n",
    "sigma_x = 0.4 \n",
    "mu_y = sp.lambdify(x,function,'numpy')(mu_x)\n",
    "sigma_y = sp.lambdify(x,derivative,'numpy')(mu_x) * sigma_x\n",
    "print(f\"The random variable x has a mean value of {mu_x} and a spread of {sigma_x}\")\n",
    "print(f\"Given the expression y={function}, the mean of the result is {mu_y} and its associated spread is {sigma_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Propagate Errors for Functions of Multiple Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we have a function that depends on multiple random variables? Given the expression $z=f(x,y,...)$, and assuming that the random variables are **uncorrelated**, the spread on the result is $\\sigma_z^2 = \\left|\\frac{\\partial f}{\\partial x}\\right|^2\\sigma_x^2 + \\left|\\frac{\\partial f}{\\partial y}\\right|^2\\sigma_y^2 + ...$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple case of the sum of two random variables, doing the math on the general formula reported above shows that the variance of the result is the sum of the variances of the random variables, i.e. $\\sigma_{x+y}^2 = \\sigma_x^2 + \\sigma_y^2$. In fact, by generalizing this formula to the sum of $N$ random variables taken from the same distribution, we can easily get the expression of the standard error of the mean. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know how to compute partial derivatives for functions of multiple variables: \n",
    "1. Refresh your upper-level Calculus \n",
    "\n",
    "or\n",
    "\n",
    "2. Use `SymPy` to do the math for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sp.symbols(['x','y'])\n",
    "function = 'x*y' \n",
    "partialf_dx = sp.diff(function,x) \n",
    "partialf_dy = sp.diff(function,y)\n",
    "# first variable, mean and spread\n",
    "mu_x = 0.2\n",
    "sigma_x = 0.1\n",
    "# second variable, mean and spread\n",
    "mu_y = 3.\n",
    "sigma_y = 1.\n",
    "# result, mean and spread\n",
    "mu_z = sp.lambdify([x,y],function,'numpy')(mu_x,mu_y)\n",
    "sigma_z = np.sqrt( (sp.lambdify([x,y],partialf_dx,'numpy')(mu_x,mu_y))**2 * sigma_x**2 \\\n",
    "                  + (sp.lambdify([x,y],partialf_dy,'numpy')(mu_x,mu_y))**2 * sigma_y**2)\n",
    "print(f\"Given two random variables x = {mu_x} and y = {mu_y} and the expression z={function},\\\n",
    "      \\nthe mean of the result is z={mu_z} and its associated spread is {sigma_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to check what happens if instead of summing two different variables, we sum the same variable twice, $z=x+x$. Is the result of the error propagation formula still consistent with the formula we derived in the previous section for a function of a single variable? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Perform Linear Regression wrt a Single Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two variables are linked by a relationship, we can try to use one to predict the other. There are several different ways we can try to find and characterize relationships between two variables. Some approaches are very flexible and don't make any assumption on the type of relation, while other approaches (called parametric) introduce some kind of analytical relation and use the available data to fit its parameters. The simplest relation between two variables is a linear model: $$Y=aX+b.$$ In general, in regression model we distinguish between independent variable(s) ($X1$, $X2$, $X3$, ..., sometimes called features) and dependent variable ($Y$, sometime called label). The distinction is generally motivated by practical reasons, usually an accurate measure of $Y$ is much more complicated than measuring the variables $X1$, $X2$, ..., which is why we use the latter to predict the former. \n",
    "\n",
    "Let us assume for simplicity that we are only interested in a single independent variable ($X$) and we want to use it to predict a target dependent variable ($Y$). We will collect $N$ datapoints (for each datapoint $i$ we will have a pair of values ${X_i,Y_i}$). Given an analytical model, such as the linear relation above, the task of machine learning is to use the available datapoints to **fit** the model. Fitting the model means getting the *best* values of the model's parameters, in this case the vaues of $a$ and $b$, where best usually is defined in a Least Squares way. Once we know the model's parameters, we can use the formula of the model to compute (**predict**) the value of $Y$ for any given value of the variable $X$, even those that are not present in our datapoints (interpolation or extrapolation). If we use the model to predict the values of $Y$ for exactly the values of $X$ in the dataset, we can compute how good the model is on the training set (i.e., its **score**).\n",
    "\n",
    "These steps are exactly reflected in the strategy used by the machine-learning algorithms in the Scikit-Learn library, also known as `Sklearn`. In the following we will see how to use `Sklearn` to perform a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import and check your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='protein.csv'\n",
    "data=pd.read_csv(path+file)\n",
    "data.plot.scatter('SUR_CONT_10.0','VOL_CONT_5.0')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create an instance of a `LinearRegression` algorithm with default settings (no arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression() # note that we can use any name we like on the left-hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lr` object created above is the one we will use to perform all the actions involved in the linear regresion. It will also store all the important quantities related to the model, in particular the values of the slope, $a$, and of the intercept, $b$, that best fit the data. However, at this stage the object has not seen the data yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create a `Numpy` array with the values of the independent variable, $X$, and another `Numpy` array with the corresponding values of the dependent variable, $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['VOL_CONT_5.0'].values # the values attribute of the DataFrame extracts only the column values\n",
    "x=data['SUR_CONT_10.0'].values.reshape(-1,1) # if we have a single independent variable X \n",
    "# we need to reshape it, because machine-learning algorithms are used to multidimensional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feed the data to the algorithm, to calculate the parameters that best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, you can see that it contains a few new attributes, which also include to the computed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The fitted linear model has an intercept of {lr.intercept_:.4f} and a slope of {lr.coef_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Plot the original y vs. x and the values predicted by the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is a simple linear model and we can access the intercept and the slope, we could just use those to calculate the equation of the fit. However, it is easier to just use the `predict()` method of the `lr` algorithm on a sensible range of values for the variable x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.linspace(400,1000,10).reshape(-1,1) # we can choose the extrema that we want and how many points we like\n",
    "y_predict = lr.predict(x_predict)\n",
    "plt.scatter(x,y,s=3,label='data')\n",
    "plt.plot(x_predict,y_predict,color='C1',label='linear regression')\n",
    "plt.xlabel('SUR_CONT_10.0')\n",
    "plt.ylabel('VOL_CONT_5.0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Compute the quality of the fit, i.e., $R^2$ value, a.k.a. the coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = lr.score(x,y)\n",
    "print(f\"The R2 coefficient of the linear fit is {R2:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coefficient describes how much of the variation in the dependent variable is actually explained by the variation in the independent variable(s). For good fits the coefficient gets close to 1, while poor fits have coefficients close to 0. It can also be computed from the data in terms of the sum of squares $R^2=1-\\frac{RSS}{TSS}$, where\n",
    "* The Total Sum of Squares (TSS) is the sum of the square of the difference of each y value from the mean value of y\n",
    "* The Residual Sum of Squares (RSS) is the sum of the square of the difference of each y value from the value predicted by the fit.\n",
    "\n",
    "The TSS tells us how much variability is present in the y variable. The RSS tells us how much residual variability is present with respect to the linear regression fit. If the fit is good, all the variability in $y$ is explained by the model, so the RSS is very small compared to the TSS and $R^2$ is close to 1. On the contrary, if there is no linear variation (i.e. the model is flat around the average $y$) the two sum of squares are identical and $R^2$ goes to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Perform Linear Regression wrt Multiple Independent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may not be very common in scientific models, it is very common in machine-learning approaches to try to make a prediction on a given dependent variable $Y$ by using many independent variables $X1$, $X2$, $X3$, etc. The idea of many of these approaches is that we can collect as much information as possible about each experiment and try to use all of it to make our prediction. This approach is reasonable when we cannot find a single good variable that is able to predict, by itself, the property that we are studying, but we know many variables that are somehow connected to such property (in a sense, you can consider this approach when standard analytical models fail and you are not able to improve on them with theory alone). \n",
    "\n",
    "With multiple independent variables, we can still write a linear model (i.e., a model linear in its parameters) as\n",
    "\n",
    "$$Y=b+a1\\cdot X1+a2\\cdot X2+a3\\cdot X3+...=b+\\mathbf{a}\\cdot\\mathbf{X},$$ \n",
    "\n",
    "where b still represents the intercept of the model (the value of $Y$ when all the independent variables are equal to 0), and we now have a slope parameter for each independent variable (for a total of M+1 parameters, with M the number of independent variables). The second formula above uses a more compact notation, in which \n",
    "\n",
    "$$\\mathbf{a}=\\left(\\begin{matrix}a1\\\\a2\\\\a3\\\\...\\end{matrix}\\right)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\mathbf{X}=\\left(\\begin{matrix}X1&X2&X3&...\\end{matrix}\\right)$$\n",
    "\n",
    "As before, we will assume that we have $N$ datapoints, and for each datapoint $i$ we collect the values of $Y_i$ and $X1_i$, $X2_i$, $X3_i$, ... We can fit a linear regression model (i.e. find the values of the parameters $b$ and $\\mathbf{a}$ that best reproduce the datapoints in a Least Square sense), following the same procedure as above. Step 1 and 2 are exactly the same as for the simpler case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data into a DataFrame\n",
    "file='protein.csv'\n",
    "data=pd.read_csv(path+file)\n",
    "# Step 2: Create an instance of a linear regression algorithm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression() # note that we can use any name we like on the left-hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Step 3, we want to use multiple columns of our `DataFrame` as our independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['VOL_CONT_5.0'].values # the values attribute of the DataFrame extracts only the column values\n",
    "x=data[['SUR_CONT_10.0','SUR_CONT_5.0']].values # since we are using multiple columns, we do NOT need the .reshape() part anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 is ideantical to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fite the model\n",
    "lr.fit(x,y)\n",
    "# you can check the intercept and the slopes (plural, as now we have an array with two slopes)\n",
    "print(lr.intercept_)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting becomes more challenging when we have more than one independent variable. With 2 variables ($X1$ and $X2$), we could still plot $Y$ in a 3D plot, but with more than 2 different variables it becomes impossible. One possibility is to just compute the model prediction on the datapoints, and report a scatter plot with respect to only one of the independent variables (either $X1$ or $X2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = lr.predict(x)\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x[:,0],y,s=3,label='data')\n",
    "plt.scatter(x[:,0],y_predict,s=3,color='C1',label='linear regression')\n",
    "plt.xlabel('SUR_CONT_5.0')\n",
    "plt.ylabel('VOL_CONT_5.0')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x[:,1],y,s=3,label='data')\n",
    "plt.scatter(x[:,1],y_predict,s=3,color='C1',label='linear regression')\n",
    "plt.xlabel('SUR_CONT_10.0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Perform Non-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression, i.e. fitting a model that is linear in the values of its parameters, is a task that can be solved analytically using linear algebra. This means that we are guaranteed that there is only one set of values of the parameters that produces the best (in a Least Squares sense) fit of the datapoints. However, if we want to fit a model that is not linear in its parameters, we cannot access the solution right away and we are not guaranteed we will find it. Usually, non-linear regressions will follow an iterative algorithm, where we start from an guesstimate of the parameters and we try to improve them systematically, changing in the direction that reduces the Least Square error. \n",
    "\n",
    "Say we want to fit a model in which the dependent variable $Y$ follows an exponential decay with respect to $X$, i.e.,\n",
    "\n",
    "$$Y=A\\cdot e^{-kX}$$\n",
    "\n",
    "The two parameters of the model are the amplitude $A$ (similar to the intercept above, this is the value of $Y$ when $X=0$), and the decay rate $k$ (with units of $1/X$). You can guess the value of $k$ from a plot of the datapoints, as the function will be a factor of $e$ smaller than at the origin when $X\\approx 1/k$ (or you can find a value $X_0$ for which $Y\\approx 0$ and guess $k\\approx 4/X_0$). \n",
    "\n",
    "In the model above, $Y$ is NOT linear in $k$, so we cannot use linear regression. However, we can still perform non-linear regression using the optimization function of SciPy. The steps are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load your data into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='uv-vis.csv'\n",
    "data=pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['Time'],data['Absorbance'])\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.ylim(0.04,1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 (Optional): Select a region of the data on which to perform the fit (filter the data). From a semilog plot of the data, it seems that the data contains some transient regime at the beginning and it does not fully decay to 0 at long times. For the fit, we will only select the datapoints with times between 0.5 and 1.5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=np.split(data.query('Time > 0.5 and Time < 1.5')[['Time','Absorbance']].values,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create a Python function that corresponds to your model. It should get the value of $X$ and the model parameters in input and it should return the correponding value of $Y$ in output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential(x, a, k):\n",
    "    \"\"\" \n",
    "    Function that returns an exponentially decaying function plus offset\n",
    "    f(x) = a*e^(-k*x)\n",
    "\n",
    "    input variables\n",
    "    x: input value (units of X)\n",
    "    a: amplitude (units of absorbance or concentration)\n",
    "    k: decay rate (units of 1/X)\n",
    "    \"\"\"\n",
    "    return a * np.exp(-k*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Guesstimate the values of the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = (1., 2.) # the function is almost 1 for X=0 and the function becomes almost 0 for X=2, so A~1 and k~4/2=2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Use SciPy `optimize.curve_fit()` to perform the non-linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "params, cv = curve_fit(exponential, x.flatten(), y.flatten(), p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes right (i.e. the algorithm is able to find a minimum), the array `params` contains the optimized values of the model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a nice side result, the matrix `cv` will contain the covariances of these parameters. The diagonal values in this matrix are the variances in the optimized parameters, i.e. the squares of their associated standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, for the decay rate (the second parameter), we got a value of 1.248 with a standard deviation of 0.0037 (in units of 1/min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(cv[1,1]) # the cv matrix contains variances in the diagonal, i.e. the squares of the standard errors in the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we did for the mean of a set of values (the SEM) can use this standard errors in the parameters together with the critical values of the normal distribution (for simplicity we are assuming the fit is performed on a large dataset) to report our errors in the parameters, in this case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "confidence = 0.95\n",
    "error_ci95 = np.sqrt(cv[1,1])*norm.ppf((1+confidence)/2)\n",
    "print(f'The 95% CI of the decay rate is {error_ci95:4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: We can use the function we created to compute the model's predictions for a given range of values of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.linspace(0,4,100)\n",
    "y_predict = exponential(x_predict,params[0],params[1]) # NOTE: we are using the parameters optimized in the previous step\n",
    "plt.plot(data['Time'],data['Absorbance'],label='full data')\n",
    "plt.scatter(x,y,s=5,label='fit data',color='black')\n",
    "plt.plot(x_predict,y_predict,label='nonlinear fit')\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Compute a Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many experimental equipments will produce a signal as a function of a given variable (frequency, time, temperature, etc.). This signal may contains a baseline (maybe a straight drift or a more complicated function), on top of which we can find peaks or interesting features. The position and intensity of a peak may be affected by having the baseline super-imposed to it. Thus, it would be nice to be able to remove the baseline from our signal in a clean and fast way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the data and (if necessary) filter it in a relevant range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dsc.txt'\n",
    "data = pd.read_csv(path+file,skiprows=2,skipfooter=1,names=['Time','Heat-Flow','Ts','Tr'],sep=' +',index_col=0,engine='python') \n",
    "data.plot('Time','Heat-Flow')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the region around the first peak by filtering the data in a range between 200 and 400 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.query('Time > 200 and Time < 400').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Use the median filter from `SciPy` to compute the median of the data in a window of $N$ points. You can think at this operation like a running average of the function, with the main difference that the median will weight more the most common points. You want to choose $N$ to be large enough so that the median is not really affected by the points that belong to the peak. In our example we have 199 points, so a number smaller than this, but larger than the number of points in the peak should work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "baseline=medfilt(filtered_data['Heat-Flow'],61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(filtered_data.index,filtered_data['Heat-Flow'])\n",
    "plt.plot(filtered_data.index,baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: You can now remove the baseline from your signal, saving the result into a new column of the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Heat-Flow-Clean']=filtered_data['Heat-Flow']-baseline\n",
    "plt.plot(filtered_data['Ts'],filtered_data['Heat-Flow-Clean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Compute the Derivatives of a Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an experimental curve, being able to compute its derivative can be very useful for many analysis purposes. In fact, many special points on the curve can be identified by looking at the curve derivative, e.g., we can find local maxima and minima, inflection points, and kinks. Computing the curve derivative from discrete data (i.e. an ensemble of $x_i$ and $f(x_i)$ pairs) instead of using the analytical expression of $f(x)$ is not that hard. Indeed we can approximate the derivative at a point as the slope of the segment connecting the point with the next one. In fact, a more symmetric definition\n",
    "\n",
    "$$f'(x_i)\\approx \\frac{f(x_{i+1})-f(x_{i-1})}{x_{i+1}-x_{i-1}}$$\n",
    "\n",
    "(i.e. the average of the slopes of the segments connecting the point with the previous and next ones) is more accurate.\n",
    "\n",
    "It can also be shown that for the second derivative the following expression provides a good approximation\n",
    "\n",
    "$$f''(x_i)\\approx 2\\frac{f(x_{i+1})+f(x_{i-1})-2f(x_i)}{x_{i+1}-x_{i-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Prepare your data. The formula above only apply for data that is aquired over a regular grid of values of x. If the x variable is not equispaced, you may want to interpolate the function over a equispaced grid of points (but this goes beyond this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'bomb.csv'\n",
    "data = pd.read_csv(path+file,usecols=(1,2),names=['temperature','time'])\n",
    "x = data['time'].values\n",
    "f_of_x = data['temperature'].values\n",
    "dx = x[1]-x[0] # the spacing between x values, assuming it is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Compute the first and/or second derivative using NumPy finite differences, Numpy gradient, or Numpy's convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_dx_diff = np.zeros(f_of_x.shape)\n",
    "df_over_dx_diff[1:] = np.diff(f_of_x, 1)/dx\n",
    "d2f_over_dx2_diff = np.zeros(f_of_x.shape)\n",
    "d2f_over_dx2_diff[1:-1] = np.diff(f_of_x,2)/dx**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_dx_gradient = np.gradient(f_of_x, x)\n",
    "d2f_over_dx2_gradient = np.gradient(df_over_dx_gradient,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_dx_conv = np.zeros(f_of_x.shape)\n",
    "d2f_over_dx2_conv = np.zeros(f_of_x.shape)\n",
    "df_over_dx_conv[1:-1] = np.convolve(f_of_x,[1,0,-1],'same')[1:-1]/2/dx\n",
    "d2f_over_dx2_conv[1:-1] = np.convolve(f_of_x,[1,-2,1],'same')[1:-1]/dx**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.tight_layout()\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(x,f_of_x)\n",
    "plt.title('function')\n",
    "plt.subplot(1,3,2)\n",
    "#plt.plot(x,df_over_dx_conv)\n",
    "plt.plot(x,df_over_dx_gradient)\n",
    "#plt.plot(x,df_over_dx_diff)\n",
    "plt.title('first derivative')\n",
    "plt.subplot(1,3,3)\n",
    "#plt.plot(x,d2f_over_dx2_conv)\n",
    "plt.plot(x,d2f_over_dx2_gradient)\n",
    "#plt.plot(x,d2f_over_dx2_diff)\n",
    "plt.title('second derivative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Compute the Area Under a Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming soon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used to allow Google Colab to install the tools to convert the notebook to a pdf file\n",
    "# Un-comment the following lines when you are ready to export the pdf \n",
    "#!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
    "#!pip install pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this command to convert the finished worksheet into a pdf \n",
    "# NOTE : you may want to change the path of the file, if you are working in a different folder of the Google Drive\n",
    "#!jupyter nbconvert --no-input --to PDF \"/content/drive/MyDrive/Colab Notebooks/Data_Analysis.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
