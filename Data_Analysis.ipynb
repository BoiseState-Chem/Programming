{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Oliviero Andreussi, olivieroandreuss@boisestate.edu\n",
    "\n",
    "Boise State University, Department of Chemistry and Biochemistry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis: Plots, Errors, Fits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us import some of the main modules that we will need for this lecture. These modules have already been introduced in the previous lecture. However, in the following we will introduce some new modules, we will add more details about them in the right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now specify the local path to the folder containing your data files. Remember to put a '/' at the end of the path and double check that the path looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './' # '/content/gdrive/MyDrive/' # this is the default path of your google drive\n",
    "my_path = 'Test_Files/' # make sure you change this to the correct path of the folder with the files\n",
    "path = base_path + my_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Make a Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check that the data was read correctly. This usually involve looking at the head, tail, and info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plot using matplotlib. Remember to add axes labels using `plt.xlabel` and `plt.ylabel`. Remember to add the units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.plot(data['Time2'],data['Absorbance2'])\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify a range for x or y (zoom in or out) using `plt.xlim` and `plt.ylim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.xlim(1,4) # from 1 to 4 seconds\n",
    "plt.ylim(0.,0.3) # correspondent zoom on the y axis\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to use a log scale for the vertical axis, add a legend, add markers to the lines, specify the color of the curve and the size of the marker, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['Time'],data['Absorbance'],'o-',ms=1,color='orange',label='Set A') # semilogy uses a log scale for the y axis\n",
    "#'o-' means a round marker and a continuous line \n",
    "# ms controls the marker size\n",
    "# color is pretty intuitive\n",
    "# label adds a text to the curve\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.legend() # this will add a legend with all the specified labels\n",
    "plt.show() # in a notebook this is not essential, but it is cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Have Multiple Plots in the Same Figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Step 1 to 3 from above and make sure the individual plots are what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create multiple plots using the `plt.subplot(n,m,i)`. The command expects three integers, which specify how many rows (`m`) and columns (`n`) of subplots we want, as well as which subplot (`i`) we want to work on (ranging from 1 to m*n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6)) # if you have multiple plots you may want to change the overall dimensions of the figure\n",
    "plt.subplot(2,1,1) # 2 rows and 1 column of plots, working on the first one (top)\n",
    "plt.semilogy(data['Time'],data['Absorbance'], label='Absorbance') # semilogy uses a log scale for the y axis\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2) # 2 rows and 1 column of plots, working on the second plot (bottom)\n",
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Report Measurement Estimates and Confidence Intervals for Large Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple sources of errors affect our measurements. If these sources of errors are small and random, and in the limit of an infinite number of measurements, the results will end up being distributed according to a bell curve (a.k.a. a Gaussian or a normal distribution). The exact value will correspond to the center of the Gaussian, usually indicated by the parameter $\\mu$. The Central Limit Theorem tells us that the best estimate for our quantity is represented by the mean of the repeated measures, i.e.,  $m=\\frac{1}{N}\\sum x_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ages.csv'\n",
    "ages = pd.read_csv(path+file,names=['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Check your data, print it, look at its statistical descriptors, plot an histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ages.head())\n",
    "print(ages.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ages,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the mean and standard deviation of your sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample of measurements is not really symmetric or looking like a Gaussian, that's because we only have a finite number $N$ of measurements. This means that using the sample's mean $m$ as our estimate for the center of the Gaussian $\\mu$ comes with an error. The Central Limit Theorem tells us that the Standard Error of the Mean (SEM, or $\\sigma_m$) is given by $\\sigma_m$=$\\sigma/\\sqrt{N}$, where $\\sigma$ is the spread of our Gaussian distribution. Like for the center of the distribution, we can estimate this spread from the sample's data, $s^2=\\frac{1}{N-1}\\sum (x_i-m)^2$. Since we already used the data to estimate $\\mu$, we have one less degree of freedom at the denominator (in practice this tells us that we cannot compute the spread if we have only one datapoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method provides immediate access to both the mean and the standard deviation of the sample. We can also compute them using methods of numpy arrays as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sample = ages.values.mean()\n",
    "std_sample = ages.values.std(ddof=1) # ddof, or delta degree of freedom, is needed to have the -1 at the denominator\n",
    "N_sample = len(ages.values)\n",
    "print(mean_sample,std_sample,N_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Compute the standard deviation of the mean from the numbers above or the ones returned by `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM = std_sample/np.sqrt(N_sample)\n",
    "print(SEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but note that there is also a dedicated function in the SciPy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "sem(ages)[0] # I use the [] notation because the result of the sem() function is an array, but I want to see its only entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Identify the critical value for the chosen confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Central Limit Theorem tells us that the mean of the sample is also distributed according to a Gaussian. The confidence interval (CI) that corresponds to a confidence level $\\gamma$ of 95% tells us that 95% of the area of the Gaussian is contained between the CI values. While the CI of a Gaussian curve will depend on the mean and standard deviation of the specific Gaussian, we can use the properties of a normalized Gaussian centered on the zero (the z-distribution) and express the CI of our estimate in terms of a critical value, i.e., the corresponding interval for the z-distribution. In practice, $CI = m\\pm z_\\gamma \\sigma_m$, where $\\sigma_m$ is the SEM, and $z_\\gamma$ is the critical value that corresponds to the given confidence level. Typical confidence levels and corresponding critical values are: \n",
    "* 90% -> $z_{90}=1.64$\n",
    "* 95% -> $z_{95}=1.96$\n",
    "* 99% -> $z_{99}=2.58$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy also provides an easy way to compute critical values for arbitrary confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "confidence = 0.9999\n",
    "norm.ppf((1+confidence)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Compute the error of your measure and fix the significant figures accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "error_ci95 = sem(ages)[0]*norm.ppf((1+confidence)/2)\n",
    "print(error_ci95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The measured age is {mean_sample:4.1f} \\u00B1 {error_ci95:3.1f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Report Measurement Estimates and Confidence Intervals for Small Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a small sample, say 3 measurements instead of 20, instead of using the z-distribution one needs to use the Student's t-distribution. Steps 1 to 4 of the section above stay the same, but Step 5 now requires to identify a specific critical value. The additional challenge is the fact that the t-distribution depends on one additional parameter, the degrees of freedom of the sample (for our applications $N-1$). While normally you would have to look up critical values of t statistics on ugly tables, SciPy allows us to compute them on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "confidence = 0.95\n",
    "error_ci95 = sem(ages)[0]*t.ppf((1+confidence)/2,df=(len(ages)-1))\n",
    "print(error_ci95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The measured age is {mean_sample:2.0f} \\u00B1 {error_ci95:1.0f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval using t-distribution will always be larger than the one of a Gaussian, but the two will become more and more similar as the number of samples gets larger. In our example the difference is minimal, although it makes us report a bigger error because of rounding. Usually less than 10 samples will show a significant difference between the two approaches. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Propagate Errors for Functions of One Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simpler case of a function of a single random variable, $y=f(x)$, one can compute the spread on the result as $\\sigma_y = \\left|\\frac{df}{dx}\\right|\\sigma_x$. What is the origin of this formula?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions to load the data { display-mode: \"form\" }\n",
    "import sympy as sp\n",
    "x = sp.symbols('x')\n",
    "function = 'x**2' # @param {type:\"string\"}\n",
    "derivative = sp.diff(function,x)\n",
    "n_measures=10000\n",
    "mean_x = 2 # @param {type:\"number\"}\n",
    "std_x = 0.4 # @param {type:\"number\"}\n",
    "mu_x = mean_x\n",
    "sigma_x = std_x\n",
    "X = np.random.normal(mu_x,sigma_x,n_measures)\n",
    "funct = sp.lambdify(x, function, \"numpy\")  \n",
    "Y = funct(X)\n",
    "dfunct = sp.lambdify(x, derivative, \"numpy\")\n",
    "error_propagation = dfunct(X.mean())*X.std(ddof=1)\n",
    "print(\"The spread on the result is {:0.3f}, while the error propagation formula gives us {:.3f}\".format(Y.std(ddof=1),error_propagation))\n",
    "# Create a Figure, which doesn't have to be square.\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "# Create the main axes, leaving 25% of the figure space at the top and on the\n",
    "# right to position marginals.\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "# Create the Axes.\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "# Draw the scatter plot and marginals.\n",
    "ax.scatter(X,Y,alpha=0.1)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histx.hist(X,bins=100)\n",
    "\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "ax_histy.hist(Y, bins=100, orientation='horizontal')\n",
    "\n",
    "x=np.linspace(X.min(),X.max(),100)\n",
    "y=funct(X.mean())+dfunct(X.mean())*(x-X.mean())\n",
    "ax.plot(x,y,'red')\n",
    "\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()-X.std(), funct(X.mean()-X.std())), xycoords='data',\n",
    "            xytext=(X.mean()-X.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()+X.std(), funct(X.mean()+X.std())), xycoords='data',\n",
    "            xytext=(X.mean()+X.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()-X.std(), funct(X.mean()-X.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(X.mean()-X.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(X.mean()+X.std(), funct(X.mean()+X.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(X.mean()+X.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know how to compute the derivative of your function with respect to the random variable, you should: \n",
    "1. Refresh your Calculus (always a great idea)\n",
    "\n",
    "or\n",
    "\n",
    "2. Use SymPy, a symbolic python module that can compute the formula of the derivative for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.symbols('x')\n",
    "function = 'x**2' # you can write the expression of your function as a string\n",
    "derivative = sp.diff(function,x) # this gets the analytical expression of df/dx\n",
    "print(derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the `function` and `derivative` above are just strings, `SymPy` can convert them to Numpy functions, so you can use them to propagate your errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sp.symbols('x')\n",
    "function = 'x**2' \n",
    "derivative = sp.diff(function,x) \n",
    "mu_x = 2.\n",
    "sigma_x = 0.4 \n",
    "mu_y = sp.lambdify(x,function,'numpy')(mu_x)\n",
    "sigma_y = sp.lambdify(x,derivative,'numpy')(mu_x) * sigma_x\n",
    "print(f\"The random variable x has a mean value of {mu_x} and a spread of {sigma_x}\")\n",
    "print(f\"Given the expression y={function}, the mean of the result is {mu_y} and its associated spread is {sigma_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Propagate Errors for Functions of Multiple Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we have a function that depends on multiple random variables? Given the expression $z=f(x,y,...)$, and assuming that the random variables are **uncorrelated**, the spread on the result is $\\sigma_z^2 = \\left|\\frac{\\partial f}{\\partial x}\\right|^2\\sigma_x^2 + \\left|\\frac{\\partial f}{\\partial y}\\right|^2\\sigma_y^2 + ...$.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple case of the sum of two random variables, doing the math on the general formula reported above shows that the variance of the result is the sum of the variances of the random variables, i.e. $\\sigma_{x+y}^2 = \\sigma_x^2 + \\sigma_y^2$. In fact, by generalizing this formula to the sum of $N$ random variables taken from the same distribution, we can easily get the expression of the standard error of the mean. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know how to compute partial derivatives for functions of multiple variables: \n",
    "1. Refresh your upper-level Calculus \n",
    "\n",
    "or\n",
    "\n",
    "2. Use `SymPy` to do the math for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sp.symbols(['x','y'])\n",
    "function = 'x*y' \n",
    "partialf_dx = sp.diff(function,x) \n",
    "partialf_dy = sp.diff(function,y)\n",
    "# first variable, mean and spread\n",
    "mu_x = 0.2\n",
    "sigma_x = 0.1\n",
    "# second variable, mean and spread\n",
    "mu_y = 3.\n",
    "sigma_y = 1.\n",
    "# result, mean and spread\n",
    "mu_z = sp.lambdify([x,y],function,'numpy')(mu_x,mu_y)\n",
    "sigma_z = np.sqrt( (sp.lambdify([x,y],partialf_dx,'numpy')(mu_x,mu_y))**2 * sigma_x**2 \\\n",
    "                  + (sp.lambdify([x,y],partialf_dy,'numpy')(mu_x,mu_y))**2 * sigma_y**2)\n",
    "print(f\"Given two random variables x = {mu_x} and y = {mu_y} and the expression z={function},\\\n",
    "      \\nthe mean of the result is z={mu_z} and its associated spread is {sigma_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to check what happens if instead of summing two different variables, we sum the same variable twice, $z=x+x$. Is the result of the error propagation formula still consistent with the formula we derived in the previous section for a function of a single variable? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to... Perform Linear Regression wrt a Single Variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two variables are linked by a relationship, we can try to use one to predict the other. There are several different ways we can try to find and characterize relationships between two variables. Some approaches are very flexible and don't make any assumption on the type of relation, while other approaches (called parametric) introduce some kind of analytical relation and use the available data to fit its parameters. The simplest relation between two variables is a linear model: $Y=aX+b$. In general, in regression model we distinguish between independent variable(s) ($X_1$, $X_2$, $X_3$, ..., sometimes called features) and dependent variable ($Y$, sometime called label). The distinction is generally motivated by practical reasons, usually an accurate measure of $Y$ is much more complicated than measuring $X$, which is why we use the latter to predict the former. \n",
    "\n",
    "Given an analytical model, such as the linear relation above, the task of machine learning is to use the available data to fit the model, so that it can then be used to make predictions. These steps are exactly reflected in the strategy used by the machine-learning algorithms in the Scikit-Learn library, also known as `Sklearn`. In the following we will see how to use `Sklearn` to perform a linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import and check your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='protein.csv'\n",
    "data=pd.read_csv(path+file)\n",
    "data.plot.scatter('SUR_CONT_10.0','VOL_CONT_5.0')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create an instance of a `LinearRegression` algorithm with default settings (no arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression() # note that we can use any name we like on the left-hand side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lr` object created above is the one we will use to perform all the actions involved in the linear regresion. It will also store all the important quantities related to the model, in particular the values of the slope, $a$, and of the intercept, $b$, that best fit the data. However, at this stage the object has not seen the data yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create a `Numpy` array with the values of the independent variable, $X$, and another `Numpy` array with the corresponding values of the dependent variable, $Y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['VOL_CONT_5.0'].values # the values attribute of the DataFrame extracts only the column values\n",
    "x=data['SUR_CONT_10.0'].values.reshape(-1,1) # if we have a single independent variable X \n",
    "# we need to reshape it, because machine-learning algorithms are used to multidimensional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Feed the data to the algorithm, to calculate the parameters that best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, you can see that it contains a few new attributes, which also include to the computed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The fitted linear model has an intercept of {lr.intercept_:.4f} and a slope of {lr.coef_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Plot the original y vs. x and the values predicted by the linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this is a simple linear model and we can access the intercept and the slope, we could just use those to calculate the equation of the fit. However, it is easier to just use the `predict()` method of the `lr` algorithm on a sensible range of values for the variable x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predict = np.linspace(400,1000,10).reshape(-1,1) # we can choose the extrema that we want and how many points we like\n",
    "y_predict = lr.predict(x_predict)\n",
    "plt.scatter(x,y,s=3,label='data')\n",
    "plt.plot(x_predict,y_predict,color='C1',label='linear regression')\n",
    "plt.xlabel('SUR_CONT_10.0')\n",
    "plt.ylabel('VOL_CONT_5.0')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Compute the quality of the fit, i.e., $R^2$ value, a.k.a. the coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = lr.score(x,y)\n",
    "print(f\"The R2 coefficient of the linear fit is {R2:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coefficient describes how much of the variation in the dependent variable is actually explained by the variation in the independent variable(s). For good fits the coefficient gets close to 1, while poor fits have coefficients close to 0. It can also be computed from the data in terms of the sum of squares $R^2=1-\\frac{RSS}{TSS}$, where\n",
    "* The Total Sum of Squares (TSS) is the sum of the square of the difference of each y value from the mean value of y\n",
    "* The Residual Sum of Squares (RSS) is the sum of the square of the difference of each y value from the value predicted by the fit.\n",
    "\n",
    "The TSS tells us how much variability is present in the y variable. The RSS tells us how much residual variability is present with respect to the linear regression fit. If the fit is good, all the variability in $y$ is explained by the model, so the RSS is very small compared to the TSS and $R^2$ is close to 1. On the contrary, if there is no linear variation (i.e. the model is flat around the average $y$) the two sum of squares are identical and $R^2$ goes to zero. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
