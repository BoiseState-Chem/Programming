{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Oliviero Andreussi, olivieroandreuss@boisestate.edu\n",
    "\n",
    "Boise State University, Department of Chemistry and Biochemistry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Data with Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us import some of the main modules that we will need for this lecture. These modules have already been introduced in the previous lecture. However, in the following we will introduce some new modules, we will add more details about them in the right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notebooks are meant to be run using Google Collaboratory (a.k.a. `Colab`). `Colab` allows you to access files stored on your Google Drive, so the rest of the notebook and all of future notebooks and worksheets will assume you have your data files stored in a subfolder of your Google Drive. The following commands will only work if you are using `Colab` and will set the `base_path` variable to the path of the main folder of your Google Drive. If you are running this notebook through `Jupyter` or using a Python IDE (e.g. VSCode), do NOT run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now specify the local path to the folder containing your data files. Remember to put a '/' at the end of the path and double check that the path looks right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/content/gdrive/MyDrive/' # this is the default path of your google drive\n",
    "my_path = 'Colab Notebooks/Test_Files/' # make sure you change this to the correct path of the folder with the files\n",
    "path = base_path + my_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard files that we would use are called comma-separated values, a.k.a. csv files. If you open one of these files, you can see they contains rows of data, where each row has the same number of values (columns) separated by a comma. A typical csv file will look like the following:  \n",
    "\n",
    "0.1, 0.0003, 0.5555,,,,<br/>\n",
    "0.2, 0.0003, 0.6666,,,,<br/>\n",
    "0.3, 0.0002, 0.7777,,,,<br/>\n",
    "..., ..., ...,,,,<br/>\n",
    "\n",
    "While many software tools and experimental devices will generate CSV files as output, you can also convert an Excel spreadsheet into a CSV file. Assuming you have a clean spreadsheet with just the data organized in columns, you can click on the Save As... option and select CSV as the file format. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can use Numpy to read text data files, there is a very powerful alternative from a different module of Python, called `Pandas`. `Pandas` introduces a new type of object, a `DataFrame`, which is the Python equivalent of an Excel spreadsheet. `DataFrames` are just `Numpy` arrays with some additional indexing and labeling, which allow to do some operations very easily and quickly. First, we need to import the module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read a CSV file with the `read_csv()` function. `Pandas` is smart enough to understand most of the settings by itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the content of the `DataFrame` using the `head()` method to look at the first few rows of the dataset, the `tail()` method to look at the last rows, or using the `info()` method to look at a summary. Remember, since these are function specific to the `DataFrame` object, they need to be called from the object itself using the `.` notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many python functions, `read_csv()` has a bunch of optional arguments that can be used to perform slightly more advanced tasks or to read files with some issues. While you may be tempted to 'fix' the file by hand, at this stage it is a good idea to try to find a Python way to read what you want from the file. This will require more time at the beginning, but it will make your life easier in the long run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you only want to read the first two colums of data from the `uv-vis.csv` file, you can do it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis.csv'\n",
    "data = pd.read_csv(path+file,usecols=[0,1]) \n",
    "# with the usecols argument you can specify a list with the numbers of the columns\n",
    "# that you want to load, in our case the first (0) and the second (1)\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the more challenging `dsc.txt` file. Don't be fooled by the different extension of this file, csv file are also text file, you can still read this file with the `read_csv()` function. However, the columns have spaces for separator, instead of commas. Also, the file has two rows for the header, as the second row contains the units, together with some special characters that are going to be problematic to read. There is also an additional line at the bottom that cotains no data, only the name of the experiment. As a minor point the first column is an index column and we could tell `Pandas` to treat it like the index of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dsc.txt'\n",
    "data = pd.read_csv(path+file,skiprows=2,skipfooter=1,names=['Time','Heat-Flow','Ts','Tr'],sep=' +',index_col=0) \n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the data separator to one or more spaces using the argument `sep=' +'` (the + sign here means one or more of the symbol before). We used `skirows` and `skipfooter` to skip the first two and the last row of the file. Since we skipped the header of the data, we provided the names of the columns using a list of names and the argument `names=[]`. Eventually, we told pandas to use the first column as index column, specifying `index_col=0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional advantage of `Pandas` is that it can also read directly Excel spreadsheets (but also tables from HTML documents and many more formats). You can check the different reading functions by typing pd.read_ and looking at the autocomplete options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'rovib.xlsx'\n",
    "data_DCl = pd.read_excel(path+file,sheet_name='DCl_0.5cm_4scans',header=3,names=['Frequency','Absorbance'])\n",
    "data_HCl = pd.read_excel(path+file,sheet_name='HCl_0.5cm_4scans',header=3,names=['Frequency','Absorbance'])\n",
    "plt.plot(data_HCl['Frequency'],data_HCl['Absorbance'],label='HCl')\n",
    "plt.plot(data_DCl['Frequency'],data_DCl['Absorbance'],label='DCl')\n",
    "plt.xlabel('Wavenumbers ($cm^{-1}$)')\n",
    "plt.ylabel('Absorbance (a.u.)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Utilities of DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` also allows us to summarize the main statistics of all the columns of the data at once, using the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also plot the data of the dataframe in an automated way, although under the hood `Pandas` is just using matplotlib. By defulat. the `DataFrame.plot()` method will plot each column as an independent set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to plot one colum vs. another one, you can specify their names in the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot('Tr','Heat-Flow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using, Adding, Dropping Columns and Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to only use a specific column of the `DataFrame` we can select it using a square braket notation and the corresponding label. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns (also known as `Pandas.Series`) are basically numpy arrays with an additional index to identify the rows. Many modules and functions can operate directly on `Series` the same way they would work on numpy arrays, say for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Ts'],data['Heat-Flow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and you can do math without having to setup loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ts']*9/5+32 # convert the temperature from C to F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the result into a new column, that is added to the `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ts_in_F']=data['Ts']*9/5+32 # we compute the temperature in F and store it in a new column\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we create new column, we can drop columns or rows that we don't need. The `.drop()` methods will automatically drop rows and you can specify them by their indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(1) # this returns a dataframe without the row with index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Ts_in_F',axis=1) # specifying axis=1 means that drop should look at columns instead of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the method return a new `DataFrame` with the dropped rows/columns, it does not modify the starting one. If you want to drop for good the rows/columns, you can just reassign the label of the old dataframe to the result of the `.drop()` operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Ts_in_F',axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can extract the numpy array component from the `Series` by using the `.values` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Ts'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way, we can use indexing to select a subset of rows from a `DataFrame`. The notation to do this is called slicing, it is similar to the `range` function, and it works the same for numpy arrays. \n",
    "array[start:end:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0:10])\n",
    "print(data[3:20:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do more advanced slicing of `DataFrames` using two special methods: `df.loc[]` and `df.iloc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[20:30,'Ts'] # loc looks at the column names and row indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[-10:-1,2] # iloc looks at the integer position of the cells of the \n",
    "# table and allows to use negative numbers to start from the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slicing synthax in the section above may be too cumbersome to use or not very effect. Often we will want to use the data itself to only select certain rows of a `DataFrame`. We can check a condition on one column and use it to filter the data. If we write a conditional rule on a column of the `DataFrame`, it will return an array of boolean results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time']>20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these results as filter to only select the rows of the dataset that evaluate to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Time']>20]\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated conditions we can use the `query()` method, to which we can pass a string with the condition as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = 10\n",
    "time_max = 50\n",
    "filtered_data=data.query(f'Time > {time_min} and Time < {time_max}')\n",
    "plt.plot(filtered_data['Time'],filtered_data['Heat-Flow'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More Challenging File to Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the more realistic and challenging `uv-vis-ugly.csv` file. If you open it you will see that the column headers are in the second row, while the first row contain the label of the experiment, which we don't need. Eventually, the last part of the file contains some text outputed by the instrument, but this is not data that we need and it is not in the same column format as the data before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try using `read_csv` with default options, the column labels will be wrong, the first row of data will have strings instead of numbers, and we will end up with three columns instead of two, since some of the last lines of the file happen to contain three commas. We can use the `skiprows` argument to specify the rows that we don't want to parse. We can easily skip the first row, and we can use `use_cols` to only load the first two columns. However, the garbage at the end of the file will mess up the last rows and, in fact, all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'uv-vis-ugly.csv'\n",
    "data = pd.read_csv(path+file,skiprows=1,usecols=[0,1]).apply(pd.to_numeric,errors='coerce').dropna()\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we know that the bad rows contain strings, not number. We can apply a function to convert the cells to be numerical values, and we can tell pandas to replace problematic values with NaN (which stands for 'not a number'). If we then drop the rows that contain NaN, we get a clean dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
