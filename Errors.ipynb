{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Oliviero Andreussi, olivieroandreuss@boisestate.edu\n",
    "\n",
    "Boise State University, Department of Chemistry and Biochemistry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Errors, Probability, and Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let us import the main modules that we will need for this lecture. You will see some new modules in the list below, we will add more details in the right sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Stochastic Model of Experimental Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple model of an experimental measuring process. Let us assume that we are measuring a property $x$, whose exact value is $x_0$. However, our measure is subject to some small errors, that can randomly affect the measured result, either by adding or by subtracting a small amount $\\epsilon$: each time we take a measure of $x$ we can think that we toss a fair coin and, if we get head, we add $\\epsilon$ to the true value, otherwise we subtract $\\epsilon$ from the true value. We can code this process using a random number between 0 and 1 as our coin toss, if the random number is lower than $p=0.5$ we consider it as getting a head on the coin toss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation setup\n",
    "x0 = 1.\n",
    "epsilon = 0.01\n",
    "p_success = 0.5\n",
    "#\n",
    "# starting guess of our results\n",
    "x = x0 # we will start assuming our measure is the exact value\n",
    "# \n",
    "# we start the stochastic process by picking a random number\n",
    "cointoss = np.random.random() # no argument means we only need one random number\n",
    "if cointoss < p_success : \n",
    "    # our random number is less than 0.5, it's a head\n",
    "    x = x + epsilon\n",
    "else :\n",
    "    # our random number is greater than 0.5, it's a tail \n",
    "    x = x - epsilon\n",
    "#\n",
    "# report the result\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we repeat the measuring process multiple times, $M$, what do you expect to be the distribution of results? Think about it before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation setup\n",
    "x0 = 1.\n",
    "epsilon = 0.01\n",
    "p_success = 0.5\n",
    "n_measures = 1000 # our name for M\n",
    "#\n",
    "# starting guess of our results\n",
    "x = np.ones(n_measures) * x0\n",
    "#\n",
    "# stochastic process\n",
    "for i in range(n_measures):\n",
    "    cointoss=np.random.random() # no argument means we only need one random number\n",
    "    if cointoss < p_success : \n",
    "        # our random number is less than 0.5, it's a head\n",
    "        x[i] = x[i] + epsilon\n",
    "    else :\n",
    "        # our random number is greater than 0.5, it's a tail \n",
    "        x[i] = x[i] - epsilon\n",
    "#\n",
    "# report the results\n",
    "plt.hist(x) # this function of matplotlib.pyplot is very useful to plot distributions\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we only had one random source of errors. In general, when we perform an experimental measure we have a very large number, $N$, of sources of errors, each contributing a small random error to our measured quantity. We can extend our model above to consider multiple sources of errors as follows. For a sake of keeping the model simple, we will assume all sources of errors will have the same effect ($\\epsilon$) on the measured quantity and they are not correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation setup \n",
    "x0 = 1.\n",
    "epsilon = 0.01\n",
    "p_success = 0.5\n",
    "n_sources = 3 # this is our name for N\n",
    "# \n",
    "# starting guess of our measure\n",
    "x = x0\n",
    "#\n",
    "# stochastic process\n",
    "cointosses=np.random.random(n_sources) # we draw multiple random numbers, one for each source of error\n",
    "for cointoss in cointosses : # for each random number we check wether to add or subtract the error\n",
    "    if cointoss < p_success :\n",
    "        # our random number is less than 0.5, it's a head\n",
    "        x = x + epsilon\n",
    "    else :\n",
    "        # our random number is greater than 0.5, it's a tail \n",
    "        x = x - epsilon\n",
    "#\n",
    "# report the result\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can now repeat this process for multiple measuring attempts and collect the distribution of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation setup\n",
    "x0=1.\n",
    "epsilon=0.001\n",
    "p_success = 0.5\n",
    "n_sources = 200\n",
    "n_measures = 10000\n",
    "#\n",
    "# starting guess of our measures\n",
    "x = np.ones(n_measures) * x0\n",
    "#\n",
    "# stochastic model\n",
    "for i in range(n_measures):\n",
    "    cointosses=np.random.random(n_sources) # no argument means we only need one random number\n",
    "    for cointoss in cointosses :\n",
    "        if cointoss < p_success :\n",
    "            # our random number is less than 0.5, it's a head\n",
    "            x[i] = x[i] + epsilon\n",
    "        else :\n",
    "            # our random number is greater than 0.5, it's a tail \n",
    "            x[i] = x[i] - epsilon\n",
    "#\n",
    "binomial,bins,_=plt.hist(x,bins=n_sources+1,density=True) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CALLENGE 1: How does the measured quantity depend on the parameters of our stochastic model ($M$, $\\epsilon$, $N$, $p$)? In particular, how does the **precision** of our measure depend on the parameters? How does the **accuracy** of our measure depend on the parameters? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process described above is an example of a Bernoulli process, for which the number of successes in $N$ trials is given by the Binomial distribution. As $N$ becomes large, the Binomial distribution becomes indistingishable from a Gaussian (a.k.a. normal) distribution. For this reason, when looking at random uncorrelated measurement errors we can assume our measures are distributed according to a normal distribution. How are the parameters of a normal distribution connected to the ones of the Bernoulli process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x0 + n_sources*(2*p_success-1)*epsilon\n",
    "sigma = np.sqrt(n_sources*p_success*(1-p_success)*4)*epsilon\n",
    "xg = np.linspace(mu-4*sigma,mu+4*sigma,100)\n",
    "gaussian = 1/np.sqrt(2*np.pi)/sigma*np.exp(-(xg-mu)**2/2/sigma**2)\n",
    "plt.plot(xg,gaussian,'r')\n",
    "plt.bar((bins[:-1]+bins[1:])/2,height=binomial*(bins[1]-bins[0])/2/epsilon,width=epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we repeat our measurements, we get a better idea of the ideal distribution from which we are sampling. Cleaerly, in the limit of an infinite number of measurements, the distribution of our measures will look very similar to a Gaussian centered on the correct value. How can we use the sample of measurements to predict $x_0$? How good is our estimate and how does it depend on the number of measures? Statistics, and in particular the Central Limit Theorem, help answering these questions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the results above, we realize that the best estimate of our true value is given by the center of the distribution of measured values. There are different ways to compute the center of a distribution of values, the most effective is the mean, $\\bar{x}=\\frac{1}{N}\\sum x_i$. We can compute the mean using a simple for loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=0.0\n",
    "for value in x : \n",
    "    mean = mean + value \n",
    "mean = mean/n_measures\n",
    "print(mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can estimate the spread of our sample distribution in terms of its variance, $\\sigma^2=\\frac{1}{N-1}\\sum (x_i-\\bar{x})^2$, or its standard deviation, $\\sigma$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">CHALLENGE 2: Write a for loop to compute the variance of the measurement results using the formula above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Numpy has some shortcuts to compute statistical properties (mean, standard deviation, variance, etc.) of values stored in a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The measurements have a mean of {:.5f}, a variance of {:.5f}, and a standard deviation of {:.5f}\".format(x.mean(),x.var(ddof=1),x.std(ddof=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we passed to the variance and standard deviation a quantity called `ddof`, which stands for delta degrees of freedom, and is the number that we subtract to the total number of measures in the denominator of the formula for the variance. In this case, we set it equal to one because we are computing the variance with respect to the mean, but the mean was computed on the sample, it is not an independent parameter. Since we used the data to compute the mean, we have one less degree of freedom to compute the variance. As an alternative definition, this number enforces the fact that we cannot compute a variance for a sample with only one measurement. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just seen that, given a sample of measurements, we can estimate properties of the distribution from which the measurements are sampled. How good are our estimates? In particular, we would like to quantify the accuracy of our estimate of the mean, since this is directly connected to the true value that we want to measure. The CLT states that the mean of a sample has a Gaussian distribution with a spread of $\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{N}}$. This quantity is usually called the standard error of the mean and it is the quantity that you usually have to use to report errors and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_95percent = 1.96\n",
    "standard_error = x.std(ddof=1)/np.sqrt(n_measures)\n",
    "print(\"Our estimate of X is {:.4f} units. The 95% confidence interval is \\u00B1 {:.4f} units\".format(x.mean(),standard_error*ci_95percent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy also allows to generate samples of arbitrary sizes from several probability distributions, including binomial, Poisson, and normal distributions. These functions are in the `random` submodule of Numpy and are a bit easier to use (but completely equivalent) than the stochastic simulation that we setup above. For example, we can generate a sample from a normal distribution centered on $\\mu$ with spread $\\sigma$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 3.\n",
    "sigma = 0.4\n",
    "n_measures = 100\n",
    "A = np.random.normal(mu,sigma,n_measures)\n",
    "plt.hist(A)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that this sample comes from the correct distribution by estimating the parameters of the distribution using the `mean()` and `std()` functions on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean and spread of the distribution are {:.4f} and {:.4f}\".format(mu,sigma))\n",
    "print(\"The mean and spread of the data are {:.4f} and {:.4f}\".format(A.mean(),A.std(ddof=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these samples to visualize what happens to our errors when we start doing operations with random variables (i.e. variables that have errors). The general formula that we want to use for error propagation applies to a generic function of multiple random variables, $y=f(A,B,...)$. Given the expression of $y$, and assuming that the random variables are uncorrelated, the error on the result is $\\sigma_y^2 = \\left|\\frac{\\partial f}{\\partial A}\\right|^2\\sigma_A^2 + \\left|\\frac{\\partial f}{\\partial B}\\right|^2\\sigma_B^2 + ...$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simpler case of a function of a single random variable, $y=f(A)$, the result simplifies to $\\sigma_y = \\left|\\frac{df}{dA}\\right|\\sigma_A$. We can check some examples below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_measures=10000\n",
    "mu_a = 3.\n",
    "sigma_a = 0.4\n",
    "A = np.random.normal(mu_a,sigma_a,n_measures)\n",
    "funct = lambda x : x**2 # this is a fancy way of Python of defining a function of x in a single line\n",
    "C = funct(A)\n",
    "dfunct = lambda x : 2*x # for the error propagation we also need the derivative of the function wrt x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_propagation = dfunct(A.mean())*A.std(ddof=1)\n",
    "print(\"The error on the result is {:0.3f}, while the propagation of error formula gives us {:.3f}\".format(C.std(ddof=1),error_propagation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Figure, which doesn't have to be square.\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "# Create the main axes, leaving 25% of the figure space at the top and on the\n",
    "# right to position marginals.\n",
    "gs = fig.add_gridspec(2, 2,  width_ratios=(4, 1), height_ratios=(1, 4),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "# Create the Axes.\n",
    "ax = fig.add_subplot(gs[1, 0])\n",
    "ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "# Draw the scatter plot and marginals.\n",
    "ax.scatter(A,C,alpha=0.1)\n",
    "ax.set_xlabel('A')\n",
    "ax.set_ylabel('C')\n",
    "\n",
    "ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "ax_histx.hist(A,bins=100)\n",
    "\n",
    "ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "ax_histy.hist(C, bins=100, orientation='horizontal')\n",
    "\n",
    "x=np.linspace(A.min(),A.max(),100)\n",
    "y=funct(A.mean())+dfunct(A.mean())*(x-A.mean())\n",
    "ax.plot(x,y,'red')\n",
    "\n",
    "ax.annotate(\"\",\n",
    "            xy=(A.mean()-A.std(), funct(A.mean()-A.std())), xycoords='data',\n",
    "            xytext=(A.mean()-A.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(A.mean()+A.std(), funct(A.mean()+A.std())), xycoords='data',\n",
    "            xytext=(A.mean()+A.std(), ax.get_ylim()[1]), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(A.mean()-A.std(), funct(A.mean()-A.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(A.mean()-A.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "ax.annotate(\"\",\n",
    "            xy=(A.mean()+A.std(), funct(A.mean()+A.std())), xycoords='data',\n",
    "            xytext=(ax.get_xlim()[1], funct(A.mean()+A.std())), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"-\", linestyle=\":\")\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we have multiple random variables? We can generate multiple samples with different centers and spreads and see how error propagation applies to this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_measures = 10000\n",
    "mu_a = 0.\n",
    "sigma_a = 0.4\n",
    "mu_b = 0.\n",
    "sigma_b = 0.4\n",
    "A = np.random.normal(mu_a,sigma_a,n_measures)\n",
    "B = np.random.normal(mu_b,sigma_b,n_measures)\n",
    "C = A + B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simple case of the sum of two random variables, doing the math on the general formula reported above shows that the variance of the result is the sum of the variances of the random variables, i.e. $\\sigma_{A+B}^2 = \\sigma_A^2 + \\sigma_B^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_propagation = np.sqrt(A.var(ddof=1) + B.var(ddof=1))\n",
    "print(\"The error on the result is {:0.3f}, while the propagation of error formula gives us {:.3f}\".format(C.std(ddof=1),error_propagation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see in the histograms that the spread of the sum is not twice as big the spread of the individual random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(A,histtype='step',bins=100)\n",
    "plt.hist(B,histtype='step',bins=100)\n",
    "plt.hist(C,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, by generalizing this formula to the sum of $N$ random variables taken from the same distribution, we can easily get the expression of the standard error of the mean. \n",
    "\n",
    "It is instructive to check what happens if instead of summing two different variables, we sum the same variable, $C=A+A$\n",
    "> CHALLENGE 3: Is the result of the error propagation formula still correct? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV Files with Numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy` has some convenient functions to read data from simple text files. The standard files that we would use are called comma-separated values, a.k.a. csv files. If you open one of these files, you can see they contains rows of data, where each row has the same number of values (columns) separated by a comma. A typical csv file will look like the following:  \n",
    "\n",
    "0.1, 0.0003, 0.5555,,,,<br/>\n",
    "0.2, 0.0003, 0.6666,,,,<br/>\n",
    "0.3, 0.0002, 0.7777,,,,<br/>\n",
    "..., ..., ...,,,,<br/>\n",
    "\n",
    "While many software tools and experimental devices will generate CSV files as output, you can also convert an Excel spreadsheet into a CSV file. Assuming you have a clean spreadsheet with just the data organized in columns, you can click on the Save As... option and select CSV as the file format. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `Numpy.loadtxt()` function to read a csv file and save the numbers in a multidimensional `Numpy` array. To test the commands of the following section, you should download the test.csv file from Canvas ([here](https://boisestatecanvas.instructure.com/courses/20544/files/10023518?wrap=1)) and save it in the Google Drive folder with your Colab notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.loadtxt('test.csv',skiprows=1,delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, loadtxt() will try to read a csv file where the different columns of values are separated by a blank space. However, most csv files use a comma as delimiter, so we want to tell this explicitly to Python. Moreover, the file we are trying to read has some lables on the first row. For this reason we want to skip reading the first row. If we look at the data and check the shape of the data, we can see that we have 2433 rows and 5 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could load each column into a different array by specifying which column to read from the file, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.loadtxt('test.csv',skiprows=1,delimiter=',',usecols=0) # only read the first column of data (index=0)\n",
    "absorbance = np.loadtxt('test.csv',skiprows=1,delimiter=',',usecols=1) # only read the second column of data (index=1)\n",
    "plt.plot(times,absorbance)\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files with Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Numpy function above does the job and it is more than enough in most circumstances, there is a very powerful alternative from a different module of Python, called `Pandas`. `Pandas` introduces a new type of object, a `DataFrame`, which is the Python equivalent of an Excel spreadsheet. `DataFrames` are just `Numpy` arrays with some additional indexing and labeling, which allow to do some operations very easily and quickly. First, we need to import the module as we have seen in the last lecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read a CSV file with the `read_csv()` function. Contrary to `Numpy` and the example above, `Pandas` is smart enough to understand most of the settings by itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the content of the `DataFrame` using the `head()` method to look at the first few rows of the dataset, or using the `info()` method to look at a summary. Remember, since these are function specific to the `DataFrame` object, they need to be called from the object itself using the `.` notation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` also allows us to summarize the main statistics of all the columns of the data at once, using the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to only use a specific column of the `DataFrame` we can select it using a square braket notation and the corresponding label. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['Time'],data['Absorbance'])\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way, we can use indexing to select a subset of rows from a `DataFrame`. Alternatively, we can use conditional rules to only select certain rows of a `DataFrame`. A conditional rule will return an array of boolean results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time']>0.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these results as filter to only select the rows of the dataset that evaluate to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Time']>0.6]\n",
    "filtered_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated conditions we can use the `query()` method, to which we can pass a string with the condition as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = 0.6\n",
    "time_max = 3.8\n",
    "filtered_data=data.query('Time > {} and Time < {}'.format(time_min,time_max))\n",
    "plt.semilogy(filtered_data['Time'],filtered_data['Absorbance'])\n",
    "plt.xlabel('Time (min)')\n",
    "plt.ylabel('Absorbance (arbitrary units)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional advantage of `Pandas` is that it can also read directly Excel spreadsheets (but also tables from HTML documents and many more formats). You can check the different reading functions by typing pd.read_ and looking at the autocomplete options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('test.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `Numpy`, we can perform math operations on a column of a `DataFrame` directly, without the need to create a `for` loop. We can also save the result into a new column, that is added to the `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['New']=np.log(data['Absorbance']) # we compute the log of the absorbance and store it in a new column called New\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CHALLENGE 4: The data that we used clearly looks like an exponential decaying function. Can you guess the parameters of the exponential decay by looking at the plot? Try to plot your guess together with the data, adjust the parameters until you get a good fit. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Scikit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two variables are linked by a relationship, we can try to use one to predict the other. There are several different ways we can try to find and characterize relationships between two variables. Some approaches are very flexible and don't make any assumption on the type of relation, while other approaches (called parametric) introduce some kind of analytical relation and use the available data to fit its parameters. The simplest relation between two variables is a linear model: $Y=aX+b$. In general, in regression model we distinguish between independent variable(s) ($X_1$, $X_2$, $X_3$, ..., sometimes called features) and dependent variable ($Y$, sometime called label). The distinction is generally motivated by practical reasons, usually an accurate measure of $Y$ is much more complicated than measuring $X$, which is why we use the latter to predict the former. \n",
    "\n",
    "Given an analytical model, such as the linear relation above, the task of machine learning is to use the available data to fit the model, so that it can then be used to make predictions. These steps are exactly reflected in the strategy used by the machine-learning algorithms in the Scikit-Learn library, also known as `Sklearn`. In the following we will see how to use `Sklearn` to perform a linear regression. Since we only want to use that specific method, we only need to import the `LinearRegression` object from the module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create an instance of a `LinearRegression` model. This step may seem useless or confusing, what we are doing is to setup a linear regression model and we are going to use the default settings for it. The object that we are creating will perform all the actions required to do a linear regression and it will store all the important quantities related to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression() # note that we can use any name we like for the instance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our algorithm we need to choose the variables we want to use as features (independent) and the one we want to predict (label or dependent). In this example, we will try to fit the logarithmic decay of the absorbance as a function of time: our dependent variable is log(Absorbance), while our independent variable is Time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.log(filtered_data['Absorbance'].values) # the values attribute of the DataFrame extracts only the column values\n",
    "features=filtered_data['Time'].values.reshape(-1,1) # we need to reshape because machine-learning algorithms are used to multidimensional features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Technical Details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the commands above we had to be a bit more explicit with the format of our data. Here are some technical details, you probably will not need to know them, but it does not hurt to have them as a reference. `Sklearn` expects that data to process to be in the form of numpy arrays. We can exctract a numpy array with the values of the columns of a `DataFrame` by specifying that we only want the `.values` of the selected column: we are basically telling `Pandas` that we do not need all the other information (the label of the column, the index of each row, etc.). If our data is already in the form of numpy arrays, we do NOT need to do anything with them, they can be used right away from `Sklearn`. \n",
    "\n",
    "However, most machine-learning algorithms will expect to have multiple features, i.e. multiple independent variables to predict a dependent variable. This corresponds to a numpy array that looks like a matrix, with one row for each datapoint, and one column for each feature. In this example we only want to use one independet variable, but we still need to pass this variable as if it was a matrix object (it is just a special matrix with just one column). In order to do that we need to use the `.reshape(-1,1)` function. We can see the difference by looking at the shape of the two objects. NOTE: if we started from a group of multiple variables we would not have needed to reshape it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the feature before reshaping is {}, while after the reshaping is {}\".format(filtered_data['Time'].values.shape,features.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit, Predict, and Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our features and label to fit our linear model, i.e. to compute the parameters (slope and intercept) that enter in the equation reported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(features,label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been fit, you can see that it contains a few new attributes, which also include to the computed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The fitted linear model has an intercept of {:.4f} and a slope of {:.4f}\".format(linear_regression.coef_[0],linear_regression.intercept_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the slope is in fact an array of slopes (in this case with just one entry), as it is usually associated with multiple independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CHALLENGE 5: Plot the data together with the best (in the least squares sense) fit. The fit will probably NOT look very good, why? Can you do better with linear regression? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression algorithm can be used to make predictions on new datapoints, i.e. if we pass a new value for the feature the algorithm can tell us the corresponding label. For example, we could try to see what happens for very long times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_times=np.linspace(3.6,6.0,100).reshape(-1,1)\n",
    "predicted_log_absorbance=linear_regression.predict(new_times)\n",
    "plt.plot(new_times,predicted_log_absorbance)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how well the fit works, we can use the linear regression object to compute a score. While scores should be used with a grain of salt, they can provide some qualitative description of good vs. bad fits. In the linear regression algorithm the typical score is the coefficient of determination, a.k.a. $R^2$. This coefficient describes how much of the variation in the dependent variable is actually explained by the variation in the independent variable(s). For good fits the coefficient gets close to 1, while poor fits have coefficients close to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(linear_regression.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The R2 coefficient of the linear fit is {:.4f}\".format(linear_regression.score(features,label)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
